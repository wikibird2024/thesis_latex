\section{Cơ sở lý thuyết về thị giác máy tính và Pose Estimation}

Sự phát triển mạnh mẽ của các thuật toán học sâu và thị giác máy tính đã mở ra những khả năng mới trong việc phát hiện và phân tích tư thế con người theo thời gian thực. Mục này đi sâu vào cơ sở lý thuyết của \textbf{MediaPipe Pose Estimation}, một giải pháp tiên tiến và tối ưu cho ứng dụng thời gian thực. Nội dung sẽ phân tích chuyên sâu về kiến trúc, các thuật toán cốt lõi và các metrics đánh giá hiệu suất, làm nền tảng vững chắc cho việc xây dựng thuật toán phát hiện té ngã. Đồng thời, mục này cũng thảo luận về các hạn chế và khả năng triển khai thực tế của hệ thống trên các thiết bị nhúng.

\subsection{Phân tích chuyên sâu về MediaPipe Pose Estimation}

Mục này tập trung vào khảo sát khả năng của \textbf{MediaPipe Pose Estimation} trong việc trích xuất khung xương người từ luồng video, phân tích hiệu suất, độ chính xác và khả năng triển khai trên các thiết bị nhúng. Chúng ta sẽ làm rõ các thuật toán cốt lõi, công thức toán học và pipeline xử lý bên trong MediaPipe, đồng thời đưa ra so sánh định lượng với các giải pháp khác.

\subsubsection{Cơ chế hoạt động và Pipeline của MediaPipe Pose Estimation}

\textbf{MediaPipe Pose Estimation} của Google là một giải pháp ước tính tư thế con người dựa trên học sâu, được thiết kế để mang lại hiệu suất cao và khả năng hoạt động thời gian thực trên nhiều nền tảng. Pipeline của nó được xây dựng trên kiến trúc \textbf{graph-based processing}, cho phép định nghĩa linh hoạt các luồng dữ liệu và các module xử lý. 

\paragraph{Kiến trúc Pipeline tổng thể}

Pipeline của MediaPipe Pose được biểu diễn dưới dạng một đồ thị (\textbf{MediaPipe Graph}), nơi các nút (nodes) đại diện cho các module xử lý (calculators) và các cạnh (edges) biểu thị luồng dữ liệu (packets). Điều này cho phép sự linh hoạt cao trong việc kết hợp các thuật toán khác nhau. Pipeline điển hình cho ước tính tư thế bao gồm ba mô hình học sâu chính, hoạt động theo chuỗi để đạt được độ chính xác và hiệu suất tối ưu:

\subparagraph{Mô hình phát hiện tư thế (Pose Detection Model):} Đây là giai đoạn đầu tiên trong pipeline, có nhiệm vụ xác định vị trí cơ thể con người trong khung hình và cung cấp một vùng quan tâm (ROI - Region of Interest). Mô hình sử dụng kiến trúc CNN nhẹ, tương tự như các mô hình trong họ BlazeFace hoặc BlazePalm, được huấn luyện để xác định bounding box của người từ khung hình RGB đầu vào.

\subparagraph{Mô hình điểm mốc tư thế (Pose Landmark Model - BlazePose):} Đây là mô hình chính trong pipeline. Từ ROI của người đã được phát hiện, mô hình sẽ dự đoán vị trí chi tiết của 33 keypoints 3D (landmarks) trên cơ thể. BlazePose là một kiến trúc CNN được tối ưu hóa, sử dụng các khối xây dựng hiệu quả như Depthwise Separable Convolutions để giảm số lượng tham số và tính toán. Kiến trúc BlazePose thường có cấu trúc encoder-decoder hoặc một phiên bản tùy chỉnh của MobileNetV3 làm backbone. Mô hình này nhận vào vùng ROI đã được thay đổi kích thước và chuẩn hóa, sau đó đưa ra tập hợp 33 keypoints $L = \{(x_k, y_k, z_k, c_k)\}_{k=1}^{33}$, với $x_k, y_k$ là tọa độ 2D, $z_k$ là tọa độ sâu tương đối, và $c_k$ là độ tin cậy của keypoint $k$.

\subparagraph{Mô hình theo dõi tư thế (Pose Tracking Model):} Sau khi một người được phát hiện, mô hình này sử dụng các thông tin từ khung hình trước đó (bounding box và keypoints) để dự đoán vị trí ROI cho khung hình hiện tại. Điều này giúp giảm chi phí tính toán đáng kể so với việc chạy lại mô hình phát hiện đầy đủ ở mỗi khung hình. Mô hình sử dụng một kiến trúc nhỏ hơn, nhanh hơn để chỉ định vị lại ROI, và nếu độ tin cậy theo dõi giảm xuống thấp, nó sẽ kích hoạt lại mô hình phát hiện đầy đủ.

\subsubsection{Thuật toán tiền xử lý và hậu xử lý}

Để tối ưu hóa hiệu suất và độ chính xác, MediaPipe sử dụng các thuật toán tiền xử lý và hậu xử lý quan trọng:

\paragraph{Bộ lọc làm mịn (Smoothing Filter):}
Sau khi các keypoint được ước tính ở mỗi khung hình, một bộ lọc làm mịn temporal được áp dụng để giảm nhiễu và đảm bảo sự mượt mà của các keypoints theo thời gian. MediaPipe sử dụng \textbf{One Euro Filter}. Công thức lọc hoàn chỉnh:
\begin{equation}
\hat{x}(t) = \alpha(t) \cdot x(t) + (1 - \alpha(t)) \cdot \hat{x}(t-1)
\end{equation}
Trong đó, $\alpha$ là hệ số làm mịn, được tính toán động dựa trên tần số cắt và vận tốc thay đổi của tín hiệu, giúp bộ lọc thích ứng với chuyển động nhanh hoặc chậm.

\paragraph{Tinh chỉnh điểm mốc (Landmark Refinement):}
Trong một số phiên bản hoặc pipeline phức tạp hơn, có thể có một bước tinh chỉnh cuối cùng sử dụng một mô hình nhỏ hơn hoặc các thuật toán hình học để điều chỉnh vị trí keypoints dựa trên các ràng buộc về xương hoặc giải phẫu học.

\subsubsection{Các công thức toán học và metrics đánh giá cốt lõi}

\paragraph{Khoảng cách giữa các keypoints:}
Khoảng cách Euclidean 3D (quan trọng cho phát hiện té ngã):
\begin{equation}
d_{3D}(i,j) = \sqrt{(x_i - x_j)^2 + (y_i - y_j)^2 + (z_i - z_j)^2}
\end{equation}

\paragraph{Góc giữa các keypoints:}
Góc giữa ba điểm A, B, C (B là đỉnh):
\begin{equation}
\theta_{ABC} = \arccos\left(\frac{\vec{BA} \cdot \vec{BC}}{|\vec{BA}| \cdot |\vec{BC}|}\right)
\end{equation}

\paragraph{Hàm mất mát trong huấn luyện Landmark Model:}
Hàm mất mát cuối cùng thường là tổng trọng số của các thành phần tọa độ và độ tin cậy:
\begin{equation}
L_{\text{total}} = \lambda_1 \sum_{k=1}^{N} ||L_{p,k} - L_{gt,k}||_2^2 + \lambda_2 \left(-\sum_{k=1}^{N} [c_{gt,k} \log(c_{p,k}) + (1-c_{gt,k}) \log(1-c_{p,k})]\right)
\end{equation}

\paragraph{Object Keypoint Similarity (OKS):}
Để đánh giá chất lượng dự đoán keypoints một cách khách quan, OKS là một metric tiêu chuẩn:
\begin{equation}
OKS = \frac{\sum_i \exp\left(-\frac{d_i^2}{2s^2k_i^2}\right) \cdot \delta(v_i > 0)}{\sum_i \delta(v_i > 0)}
\end{equation}
Trong đó $d_i$ là khoảng cách giữa keypoint dự đoán và ground truth, $s$ là diện tích đối tượng, $k_i$ là hằng số trọng số của keypoint, và $\delta$ là hàm Kronecker delta.

\paragraph{Công thức tính tốc độ khung hình và ảnh hưởng của batch size}
Tốc độ khung hình (FPS) là số lượng khung hình mà hệ thống có thể xử lý trong một giây:
\begin{equation}
\text{FPS} = \frac{1}{\text{Thời gian xử lý trung bình mỗi khung hình}}
\end{equation}

\subsection{So sánh MediaPipe với các giải pháp khác}

Để đánh giá khách quan khả năng của MediaPipe Pose Estimation, chúng ta tiến hành so sánh với các giải pháp ước tính tư thế hàng đầu khác như OpenPose, MoveNet và YOLOv7-Pose. Bảng \ref{tab:comparison_pose_detailed} trình bày so sánh chi tiết giữa các giải pháp này, tập trung vào các tiêu chí kỹ thuật và khả năng triển khai thực tế.

\begin{table}[htbp]
\centering
\caption{So sánh chi tiết giữa các giải pháp Pose Estimation}
\label{tab:comparison_pose_detailed}
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{|p{2.8cm}|p{3cm}|p{2.8cm}|p{2.8cm}|p{2.8cm}|}
\hline
\textbf{Tiêu chí} & \textbf{MediaPipe Pose} & \textbf{YOLOv7-Pose} & \textbf{OpenPose} & \textbf{MoveNet} \\
\hline
\textbf{Số keypoints} & 33 (3D) & 17 (2D) & 25 (2D) & 17 (2D) \\
\hline
\textbf{Kích thước mô hình} & Detection: 815K\newline Landmarks: 3.37M & ~37M params & ~200M params & Lightning: 1.5M\newline Thunder: 7M \\
\hline
\textbf{Độ chính xác (mAP)} & 0.65-0.75 & 0.68-0.78 & 0.70-0.80 & 0.60-0.72 \\
\hline
\textbf{FPS (RTX 3080)} & 60-100 & 45-80 & 25-45 & 80-120 \\
\hline
\textbf{FPS (CPU i7)} & 25-40 & 15-25 & 8-15 & 30-60 \\
\hline
\textbf{Ước tính 3D} & Có & Không & Không & Không \\
\hline
\textbf{Tương thích} & Python, C++\newline Web, Mobile & Python, C++ & Python, C++\newline CUDA & Python, C++\newline Web, Mobile \\
\hline
\end{tabular}
\end{table}

\subsection{Phân tích khả năng triển khai trên thiết bị nhúng}

\subsubsection{ESP32}
Trực tiếp chạy các mô hình học sâu lớn như MediaPipe, OpenPose hay MoveNet trên ESP32 là không khả thi do hạn chế về tài nguyên (RAM: $\approx$ 520KB, Flash: vài MB, tần số CPU thấp) và không có bộ tăng tốc học sâu chuyên dụng. Để sử dụng ESP32, cần một kiến trúc phân tán: một thiết bị mạnh hơn (như Raspberry Pi hoặc Jetson Nano) sẽ xử lý phần thị giác và gửi dữ liệu keypoint đã trích xuất đến ESP32. ESP32 sau đó sẽ chịu trách nhiệm xử lý logic phát hiện té ngã.

\subsubsection{Raspberry Pi 4 / Jetson Nano}
Đây là các nền tảng nhúng phù hợp hơn. MediaPipe có thể chạy thời gian thực trên Raspberry Pi 4 (đặc biệt khi tối ưu với OpenCL/OpenGL ES backend) và Jetson Nano (sử dụng TensorRT hoặc GPU delegate của TF Lite). MoveNet là một lựa chọn tuyệt vời cho các thiết bị này nhờ kích thước mô hình cực kỳ nhỏ và tốc độ suy luận nhanh trên CPU.

\subsection{Các hạn chế và thách thức}

\subsubsection{Điều kiện môi trường}
MediaPipe Pose Estimation gặp khó khăn trong các điều kiện: ánh sáng yếu, backlight mạnh, shadow phức tạp, và motion blur khi chuyển động nhanh.

\subsubsection{Occlusion và đa đối tượng}
- Keypoint accuracy giảm khi bị che khuất một phần.
- MediaPipe Pose không hỗ trợ multi-person tracking một cách nguyên bản mà cần các giải pháp bổ sung.

\section{Thuật toán phát hiện té ngã dựa trên dữ liệu MediaPipe}

Phần này trình bày cách sử dụng dữ liệu keypoint 3D từ MediaPipe để xây dựng một thuật toán phát hiện té ngã đơn giản nhưng hiệu quả, kết hợp phân tích tư thế và chuyển động. 

\subsection{Logic phát hiện té ngã và các thông số kỹ thuật}

Thuật toán của đề tài hoạt động dựa trên ba giai đoạn chính:

\paragraph{Giai đoạn 1: Phát hiện cú ngã đột ngột.}
Hệ thống theo dõi tốc độ thay đổi vị trí của các keypoint trên cơ thể, đặc biệt là đầu và hông. Một cú ngã được đánh dấu khi tốc độ rơi thẳng đứng của đầu hoặc hông vượt qua một ngưỡng nhất định ($V_{th}$). Tốc độ này có thể được tính bằng cách so sánh vị trí y của keypoint đầu giữa khung hình hiện tại và khung hình trước:
\begin{equation}
V_{\text{head}} = |\text{head\_y}_t - \text{head\_y}_{t-1}|
\end{equation}

\paragraph{Giai đoạn 2: Phân tích tư thế sau cú ngã.}
Sau khi phát hiện cú ngã đột ngột, hệ thống kiểm tra xem người dùng có đang ở tư thế nằm ngang hay không. Điều này được xác định bằng cách tính toán \textbf{tỷ lệ khung hình (aspect ratio)} của cơ thể:
\begin{equation}
\text{Aspect Ratio} = \frac{\text{Chiều rộng của bounding box}}{\text{Chiều cao của bounding box}}
\end{equation}

\paragraph{Giai đoạn 3: Xác minh trạng thái bất động.}
Đây là bước quan trọng để loại bỏ các cảnh báo sai. Hệ thống sử dụng một bộ đếm thời gian ($T_{th}$). Nếu người dùng không di chuyển trong một khoảng thời gian $T_{th}$ sau khi đáp ứng cả hai điều kiện trên, hệ thống sẽ xác nhận là té ngã thực sự và kích hoạt cảnh báo.
